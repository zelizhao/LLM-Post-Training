# A Survey on Post-training of Large Language Models


[![arXiv](https://img.shields.io/badge/arXiv-2503.06072-b31b1b.svg)](https://arxiv.org/pdf/2503.06072)

<p align="center">
    <img src="https://i.imgur.com/waxVImv.png" alt="Oryx Video-ChatGPT">
</p>

Welcome to the **LLM-Post-training-Survey** repository! This repository is a curated collection of the most influential Fine-Tuning, alignment, reasoning, and efficiency related to **Large Language Models (LLMs) Post-Training  Methodologies**. 

Our work is based on the following paper:  
üìÑ **A Survey on Post-training of Large Language Models** ‚Äì Available on [![arXiv](https://img.shields.io/badge/arXiv-2503.06072-b31b1b.svg)](https://arxiv.org/pdf/2503.06072)
 
- **Corresponding authors:** [Guiyao Tie](mailto:tgy@hust.edu.cn), [Zeli zhao](mailto:zhaozeli@hust.edu.cn).  

Feel free to ‚≠ê star and fork this repository to keep up with the latest advancements and contribute to the community.

---
<p align="center">
  <img src="./Images/teasor.jpg" width="45%" hieght="50%" />
<!--   <img src="./Images/methods.jpg" width="45%" height="50%" /> -->
</p>
Structural overview of post-training techniques surveyed in this study, illustrating the organization of methodologies, datasets, and applications.

---

## üìå Contents  

| Section | Subsection |  
| ------- | ----------- |  
| [ü§ñ PoLMs for Fine-Tuning](#PoLMs-for-Fine-Tuning) | [Supervised Fine-Tuning](#Supervised-Fine-Tuning), [Adaptive Fine-Tuning](#Adaptive-Fine-Tuning), [Reinforcement Fine-Tuning](#Reinforcement-Fine-Tuning) |  
| [üèÜ PoLMs for Alignment](#PoLMs-for-Alignment) | [Reinforcement Learning with Human Feedback](#Reinforcement-Learning-with-Human-Feedback), [Reinforcement Learning with AI Feedback](#Reinforcement-Learning-with-AI-Feedback), [Direct Preference Optimization](#Direct-Preference-Optimization) |  
| [üöÄ PoLMs for Reasoning](#PoLMs-for-Reasoning) | [Self-Refine for Reasoning](#Self-Refine-for-Reasoning), [Reinforcement Learning for Reasoning](#Reinforcement-Learning-for-Reasoning) |  
| [üß† PoLMs for Efficiency](#PoLMs-for-Efficiency) | [Model Compression](#Model-Compression), [Parameter-Efficient Fine-Tuning](#Parameter-Efficient-Fine-Tuning), [Knowledge-Distillation](#Knowledge-Distillation) |  
| [üåÄ PoLMs for Integration and Adaptation](#PoLMs-for-Integration-and-Adaptation) | [Multi-Modal Integration](#Multi-Modal-Integration), [Domain Adaptation](#Domain-Adaptation), [Model Merging](#Model-Merging) |  
| [ü§ù Datasets](#Datasets) | [Human-Labeled Datasets](#Human-Labeled-Datasets), [Distilled Dataset](#Distilled-Dataset), [Synthetic Datasets](#Synthetic-Datasets) |  
| [üìö Applications](#Applications) | [Professional Domains](#Professional-Domains), [Technical and Logical Reasoning](#Technical-and-Logical-Reasoning), [Understanding and Interaction](Understanding-and-Interaction) |  


---

# üìñ Papers  


## ü§ñ PoLMs for Fine-Tuning  

* Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search [[Paper]](https://arxiv.org/abs/2502.02508) ![](https://img.shields.io/badge/arXiv-2025.02-red)

---

## üèÜ PoLMs for Alignment

* PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models. [[Paper]](https://arxiv.org/abs/2501.03124) ![](https://img.shields.io/badge/arXiv-2025.01-red)

---
## üöÄ PoLMs for Reasoning
* On the Convergence Rate of MCTS for the Optimal Value Estimation in Markov Decision Processes [[Paper]](https://ieeexplore.ieee.org/abstract/document/10870057/) ![](https://img.shields.io/badge/IEEE_TAC-2025-blue)

---

## üß† PoLMs for Efficiency
* Agents Thinking Fast and Slow: A Talker-Reasoner Architecture [[Paper]](https://openreview.net/forum?id=xPhcP6rbI4) ![](https://img.shields.io/badge/NeurIPS_WorkShop-2024-blue)

## üåÄ PoLMs for Integration and Adaptation
* Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable [[Paper]](https://arxiv.org/abs/2503.00555v1) ![](https://img.shields.io/badge/arXiv-2025.03-red)


---

## ü§ù Datasets 

* Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models [[Paper]](https://arxiv.org/abs/2502.17387) ![](https://img.shields.io/badge/arXiv-2025.02-red)

## üìö  Applications

* Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models [[Paper]](https://arxiv.org/abs/2502.17387) ![](https://img.shields.io/badge/arXiv-2025.02-red)
---

## üìå Contributing  

Contributions are welcome! If you have relevant papers, code, or insights, feel free to submit a pull request.  

[![Star History Chart](https://api.star-history.com/svg?repos=mbzuai-oryx/Awesome-LLM-Post-training&type=Timeline)](https://www.star-history.com/#mbzuai-oryx/Awesome-LLM-Post-training&Timeline)

## Citation

If you find our work useful or use it in your research, please consider citing:

```bibtex
@inproceedings{Tie2025ASO,
  title={A Survey on Post-training of Large Language Models},
  author={Guiyao Tie and Zeli Zhao and Dingjie Song and Fuyang Wei and Rong Zhou and Yurou Dai and Wen Yin and Zhejian Yang and Jiangyue Yan and Yao Su and Zhenhan Dai and Yifeng Xie and Yihan Cao and Lichao Sun and Pan Zhou and Lifang He and Hechang Chen and Yu Zhang and Qingsong Wen and Tianming Liu and Neil Zhenqiang Gong and Jiliang Tang and Caiming Xiong and Heng Ji and Philip S. Yu and Jianfeng Gao},
  year={2025},
  url={https://api.semanticscholar.org/CorpusID:276902416}
}
```
